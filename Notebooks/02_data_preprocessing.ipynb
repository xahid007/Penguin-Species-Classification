{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset and prepare the training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>culmen_length_mm</th>\n",
       "      <th>culmen_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  culmen_length_mm  culmen_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen              39.1             18.7              181.0   \n",
       "1  Adelie  Torgersen              39.5             17.4              186.0   \n",
       "2  Adelie  Torgersen              40.3             18.0              195.0   \n",
       "3  Adelie  Torgersen               NaN              NaN                NaN   \n",
       "4  Adelie  Torgersen              36.7             19.3              193.0   \n",
       "\n",
       "   body_mass_g     sex  \n",
       "0       3750.0    MALE  \n",
       "1       3800.0  FEMALE  \n",
       "2       3250.0  FEMALE  \n",
       "3          NaN     NaN  \n",
       "4       3450.0  FEMALE  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Dataset/penguins_size.csv')\n",
    "df = df[df['sex'] != '.'] #There is a row where sex = '.', so filtering it out\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (343, 6)\n",
      "Shape of y: (343,)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['species'])\n",
    "y  = df.species\n",
    "print('Shape of X:', X.shape)\n",
    "print('Shape of y:', y.shape)\n",
    "\n",
    "num_features = X.select_dtypes(include='number').columns\n",
    "cat_features = X.select_dtypes(include='object').columns\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missinng value Handeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species               0\n",
       "island                0\n",
       "culmen_length_mm      2\n",
       "culmen_depth_mm       2\n",
       "flipper_length_mm     2\n",
       "body_mass_g           2\n",
       "sex                  10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach for handelling missing value:\n",
    "\n",
    "1. For the `sex` column (categorical feature), replace any missing values with the most frequently occurring value (the mode).\n",
    "2. For the remaining numerical features with missing values, use the average value (mean) of each respective column to fill in the gaps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_impute = SimpleImputer(strategy='mean')\n",
    "cat_impute = SimpleImputer(strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[num_features] = num_impute.fit_transform(X_train[num_features])\n",
    "X_test[num_features]  = num_impute.fit_transform(X_test[num_features])\n",
    "\n",
    "X_train[cat_features]  = cat_impute.fit_transform(X_train[cat_features])\n",
    "X_test[cat_features] = cat_impute.fit_transform(X_test[cat_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "island               0\n",
       "culmen_length_mm     0\n",
       "culmen_depth_mm      0\n",
       "flipper_length_mm    0\n",
       "body_mass_g          0\n",
       "sex                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "island               0\n",
       "culmen_length_mm     0\n",
       "culmen_depth_mm      0\n",
       "flipper_length_mm    0\n",
       "body_mass_g          0\n",
       "sex                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train[num_features]  = scaler.fit_transform(X_train[num_features])\n",
    "X_test[num_features]  = scaler.transform(X_test[num_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach:**\n",
    "- Apply one-hot encoding to categorical columns (`Sex`, `island`, and `species`) because these columns do not have an inherent order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Use OneHotEncoder for categorical features\n",
    "categorical_cols = ['sex', 'island']\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialize the OneHotEncoder (use sparse_output instead of sparse)\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' to avoid multicollinearity\n",
    "\n",
    "# Fit the encoder on the training data\n",
    "X_train_encoded = encoder.fit_transform(X_train[categorical_cols])\n",
    "\n",
    "# Transform the test data using the encoder fitted on X_train\n",
    "X_test_encoded = encoder.transform(X_test[categorical_cols])\n",
    "\n",
    "# Get the column names for the encoded variables\n",
    "encoded_columns = encoder.get_feature_names_out(categorical_cols)\n",
    "\n",
    "# Convert the encoded arrays to DataFrames\n",
    "X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_columns)\n",
    "X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_columns)\n",
    "\n",
    "# Combine the original non-categorical columns with the encoded columns\n",
    "X_train_final = pd.concat([X_train.drop(columns=categorical_cols).reset_index(drop=True), X_train_encoded_df], axis=1)\n",
    "X_test_final = pd.concat([X_test.drop(columns=categorical_cols).reset_index(drop=True), X_test_encoded_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>culmen_length_mm</th>\n",
       "      <th>culmen_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex_MALE</th>\n",
       "      <th>island_Dream</th>\n",
       "      <th>island_Torgersen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.194968</td>\n",
       "      <td>8.603480e-01</td>\n",
       "      <td>-1.570186e+00</td>\n",
       "      <td>-1.541421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.811698</td>\n",
       "      <td>1.756003e+00</td>\n",
       "      <td>-7.046080e-01</td>\n",
       "      <td>-0.383053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.767780e-15</td>\n",
       "      <td>2.050100e-15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.192105</td>\n",
       "      <td>-1.329030e+00</td>\n",
       "      <td>1.026548e+00</td>\n",
       "      <td>0.994465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.265109</td>\n",
       "      <td>-8.506524e-02</td>\n",
       "      <td>-3.439505e-01</td>\n",
       "      <td>-0.883969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   culmen_length_mm  culmen_depth_mm  flipper_length_mm  body_mass_g  \\\n",
       "0         -1.194968     8.603480e-01      -1.570186e+00    -1.541421   \n",
       "1         -0.811698     1.756003e+00      -7.046080e-01    -0.383053   \n",
       "2          0.000000     1.767780e-15       2.050100e-15     0.000000   \n",
       "3          0.192105    -1.329030e+00       1.026548e+00     0.994465   \n",
       "4          0.265109    -8.506524e-02      -3.439505e-01    -0.883969   \n",
       "\n",
       "   sex_MALE  island_Dream  island_Torgersen  \n",
       "0       1.0           1.0               0.0  \n",
       "1       0.0           0.0               0.0  \n",
       "2       1.0           0.0               1.0  \n",
       "3       0.0           0.0               0.0  \n",
       "4       0.0           1.0               0.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier,AdaBoostClassifier, BaggingClassifier\n",
    "# import xgboost as xgb\n",
    "# import lightgbm as lgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, n_jobs=-1),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(n_jobs=-1),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    #'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_jobs=-1),\n",
    "    'Bagging': BaggingClassifier(n_jobs=-1),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    # 'LightGBM': lgb.LGBMClassifier(random_state = 0)\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "metrics = {\n",
    "    'Model': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1 Score': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       1.00      1.00      1.00        30\n",
      "   Chinstrap       1.00      1.00      1.00        14\n",
      "      Gentoo       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           1.00        69\n",
      "   macro avg       1.00      1.00      1.00        69\n",
      "weighted avg       1.00      1.00      1.00        69\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       1.00      1.00      1.00        30\n",
      "   Chinstrap       1.00      1.00      1.00        14\n",
      "      Gentoo       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           1.00        69\n",
      "   macro avg       1.00      1.00      1.00        69\n",
      "weighted avg       1.00      1.00      1.00        69\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       1.00      1.00      1.00        30\n",
      "   Chinstrap       1.00      1.00      1.00        14\n",
      "      Gentoo       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           1.00        69\n",
      "   macro avg       1.00      1.00      1.00        69\n",
      "weighted avg       1.00      1.00      1.00        69\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       1.00      1.00      1.00        30\n",
      "   Chinstrap       1.00      1.00      1.00        14\n",
      "      Gentoo       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           1.00        69\n",
      "   macro avg       1.00      1.00      1.00        69\n",
      "weighted avg       1.00      1.00      1.00        69\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       1.00      0.57      0.72        30\n",
      "   Chinstrap       0.54      1.00      0.70        14\n",
      "      Gentoo       0.96      1.00      0.98        25\n",
      "\n",
      "    accuracy                           0.81        69\n",
      "   macro avg       0.83      0.86      0.80        69\n",
      "weighted avg       0.89      0.81      0.81        69\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       1.00      1.00      1.00        30\n",
      "   Chinstrap       1.00      1.00      1.00        14\n",
      "      Gentoo       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           1.00        69\n",
      "   macro avg       1.00      1.00      1.00        69\n",
      "weighted avg       1.00      1.00      1.00        69\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       0.97      0.93      0.95        30\n",
      "   Chinstrap       0.88      1.00      0.93        14\n",
      "      Gentoo       1.00      0.96      0.98        25\n",
      "\n",
      "    accuracy                           0.96        69\n",
      "   macro avg       0.95      0.96      0.95        69\n",
      "weighted avg       0.96      0.96      0.96        69\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       1.00      1.00      1.00        30\n",
      "   Chinstrap       1.00      1.00      1.00        14\n",
      "      Gentoo       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           1.00        69\n",
      "   macro avg       1.00      1.00      1.00        69\n",
      "weighted avg       1.00      1.00      1.00        69\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       1.00      0.97      0.98        30\n",
      "   Chinstrap       0.93      1.00      0.97        14\n",
      "      Gentoo       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           0.99        69\n",
      "   macro avg       0.98      0.99      0.98        69\n",
      "weighted avg       0.99      0.99      0.99        69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "        model.fit(X_train_final, y_train)\n",
    "        y_pred = model.predict(X_test_final)\n",
    "\n",
    "        # Compute metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        metrics['Model'].append(name)\n",
    "        metrics['Accuracy'].append(accuracy)\n",
    "        metrics['Precision'].append(report['weighted avg']['precision'])\n",
    "        metrics['Recall'].append(report['weighted avg']['recall'])\n",
    "        metrics['F1 Score'].append(report['weighted avg']['f1-score'])\n",
    "\n",
    "# Create DataFrame from metrics\n",
    "metrics_df = pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.811594</td>\n",
       "      <td>0.892419</td>\n",
       "      <td>0.811594</td>\n",
       "      <td>0.811767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.959645</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.956972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bagging</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.985507</td>\n",
       "      <td>0.986473</td>\n",
       "      <td>0.985507</td>\n",
       "      <td>0.985634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Accuracy  Precision    Recall  F1 Score\n",
       "0     Logistic Regression  1.000000   1.000000  1.000000  1.000000\n",
       "1           Decision Tree  1.000000   1.000000  1.000000  1.000000\n",
       "2           Random Forest  1.000000   1.000000  1.000000  1.000000\n",
       "3  Support Vector Machine  1.000000   1.000000  1.000000  1.000000\n",
       "4             Naive Bayes  0.811594   0.892419  0.811594  0.811767\n",
       "5       Gradient Boosting  1.000000   1.000000  1.000000  1.000000\n",
       "6                AdaBoost  0.956522   0.959645  0.956522  0.956972\n",
       "7                 Bagging  1.000000   1.000000  1.000000  1.000000\n",
       "8     K-Nearest Neighbors  0.985507   0.986473  0.985507  0.985634"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       1.00      0.97      0.98        30\n",
      "   Chinstrap       0.93      1.00      0.97        14\n",
      "      Gentoo       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           0.99        69\n",
      "   macro avg       0.98      0.99      0.98        69\n",
      "weighted avg       0.99      0.99      0.99        69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train_final,y_train)\n",
    "y_pred = knn.predict(X_test_final)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model  Accuracy  Precision    Recall  F1 Score\n",
      "0     Logistic Regression  0.992727   0.993007  0.992727  0.992663\n",
      "1           Decision Tree  0.956296   0.958193  0.956296  0.956172\n",
      "2           Random Forest  0.978182   0.978467  0.978182  0.977671\n",
      "3  Support Vector Machine  0.985455   0.986763  0.985455  0.984889\n",
      "4             Naive Bayes  0.802963   0.879667  0.802963  0.798693\n",
      "5       Gradient Boosting  0.974545   0.975674  0.974545  0.974200\n",
      "6                AdaBoost  0.926936   0.934856  0.926936  0.928665\n",
      "7                 Bagging  0.974545   0.975288  0.974545  0.974376\n",
      "8     K-Nearest Neighbors  0.985387   0.986307  0.985387  0.985412\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../Dataset/penguins_size.csv')\n",
    "df = df[df['sex'] != '.']  # Filter out rows where sex = '.'\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['species'])\n",
    "y = df['species']\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "num_features = X.select_dtypes(include='number').columns.tolist()\n",
    "cat_features = X.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Define preprocessing for numerical features\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values with mean\n",
    "    ('scaler', StandardScaler())  # Scale numerical features\n",
    "])\n",
    "\n",
    "# Define preprocessing for categorical features\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing values with mode\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first'))  # Encode categorical features\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps for numerical and categorical features\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, num_features),\n",
    "    ('cat', categorical_pipeline, cat_features)\n",
    "])\n",
    "\n",
    "# Define models with pipelines\n",
    "pipelines = {\n",
    "    'Logistic Regression': Pipeline([('preprocessor', preprocessor), ('model', LogisticRegression(max_iter=1000, n_jobs=-1))]),\n",
    "    'Decision Tree': Pipeline([('preprocessor', preprocessor), ('model', DecisionTreeClassifier())]),\n",
    "    'Random Forest': Pipeline([('preprocessor', preprocessor), ('model', RandomForestClassifier(n_jobs=-1))]),\n",
    "    'Support Vector Machine': Pipeline([('preprocessor', preprocessor), ('model', SVC())]),\n",
    "    'Naive Bayes': Pipeline([('preprocessor', preprocessor), ('model', GaussianNB())]),  # Note: Naive Bayes may not require scaling\n",
    "    'Gradient Boosting': Pipeline([('preprocessor', preprocessor), ('model', GradientBoostingClassifier())]),\n",
    "    'AdaBoost': Pipeline([('preprocessor', preprocessor), ('model', AdaBoostClassifier())]),\n",
    "    'Bagging': Pipeline([('preprocessor', preprocessor), ('model', BaggingClassifier(n_jobs=-1))]),\n",
    "    'K-Nearest Neighbors': Pipeline([('preprocessor', preprocessor), ('model', KNeighborsClassifier())])\n",
    "}\n",
    "\n",
    "# Initialize metrics dictionary\n",
    "metrics = {\n",
    "    'Model': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1 Score': []\n",
    "}\n",
    "\n",
    "# Perform cross-validation for each pipeline\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # Use stratified cross-validation\n",
    "\n",
    "for name, pipeline in pipelines.items():\n",
    "    scores = cross_validate(pipeline, X_train, y_train, cv=cv, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "    metrics['Model'].append(name)\n",
    "    metrics['Accuracy'].append(scores['test_accuracy'].mean())\n",
    "    metrics['Precision'].append(scores['test_precision_weighted'].mean())\n",
    "    metrics['Recall'].append(scores['test_recall_weighted'].mean())\n",
    "    metrics['F1 Score'].append(scores['test_f1_weighted'].mean())\n",
    "\n",
    "# Create DataFrame from metrics\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       1.00      1.00      1.00        30\n",
      "   Chinstrap       1.00      1.00      1.00        14\n",
      "      Gentoo       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           1.00        69\n",
      "   macro avg       1.00      1.00      1.00        69\n",
      "weighted avg       1.00      1.00      1.00        69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "best_model = pipelines['Logistic Regression']  # Replace with the chosen model\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Metrics:\n",
      "                    Model  Accuracy  Precision    Recall  F1 Score\n",
      "0     Logistic Regression  0.992727   0.993007  0.992727  0.992663\n",
      "1           Decision Tree  0.967273   0.968195  0.967273  0.967318\n",
      "2           Random Forest  0.974545   0.974624  0.974545  0.973980\n",
      "3  Support Vector Machine  0.992727   0.993007  0.992727  0.992663\n",
      "4             Naive Bayes  0.901347   0.937892  0.901347  0.904475\n",
      "5       Gradient Boosting  0.974545   0.975674  0.974545  0.974200\n",
      "6                AdaBoost  0.926936   0.934856  0.926936  0.928665\n",
      "7                 Bagging  0.974545   0.975114  0.974545  0.974419\n",
      "8     K-Nearest Neighbors  0.985387   0.986307  0.985387  0.985412\n",
      "\n",
      "Test Set Evaluation for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       1.00      0.97      0.98        30\n",
      "   Chinstrap       0.93      1.00      0.97        14\n",
      "      Gentoo       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           0.99        69\n",
      "   macro avg       0.98      0.99      0.98        69\n",
      "weighted avg       0.99      0.99      0.99        69\n",
      "\n",
      "Confusion Matrix for Logistic Regression:\n",
      "[[29  1  0]\n",
      " [ 0 14  0]\n",
      " [ 0  0 25]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../Dataset/penguins_size.csv')\n",
    "df = df[df['sex'] != '.']  # Filter out rows where sex = '.'\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['species'])\n",
    "y = df['species']\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "num_features = X.select_dtypes(include='number').columns.tolist()\n",
    "cat_features = X.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Define preprocessing for numerical features (with KNN imputation)\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),  # Use KNN imputation for numerical features\n",
    "    ('scaler', StandardScaler())  # Scale numerical features\n",
    "])\n",
    "\n",
    "# Define preprocessing for categorical features\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing values with mode\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first'))  # Encode categorical features\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps for numerical and categorical features\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, num_features),\n",
    "    ('cat', categorical_pipeline, cat_features)\n",
    "])\n",
    "\n",
    "# Define models with pipelines\n",
    "pipelines = {\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', LogisticRegression(max_iter=1000, n_jobs=-1, class_weight='balanced'))\n",
    "    ]),\n",
    "    'Decision Tree': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', DecisionTreeClassifier(class_weight='balanced'))\n",
    "    ]),\n",
    "    'Random Forest': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', RandomForestClassifier(n_jobs=-1, class_weight='balanced'))\n",
    "    ]),\n",
    "    'Support Vector Machine': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', SVC(class_weight='balanced'))\n",
    "    ]),\n",
    "    'Naive Bayes': Pipeline([\n",
    "        ('preprocessor', ColumnTransformer([\n",
    "            ('num', SimpleImputer(strategy='mean'), num_features),  # Skip scaling for Naive Bayes\n",
    "            ('cat', categorical_pipeline, cat_features)\n",
    "        ])),\n",
    "        ('model', GaussianNB())\n",
    "    ]),\n",
    "    'Gradient Boosting': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', GradientBoostingClassifier())\n",
    "    ]),\n",
    "    'AdaBoost': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', AdaBoostClassifier())\n",
    "    ]),\n",
    "    'Bagging': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', BaggingClassifier(n_jobs=-1))\n",
    "    ]),\n",
    "    'K-Nearest Neighbors': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', KNeighborsClassifier())\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Initialize metrics dictionary\n",
    "metrics = {\n",
    "    'Model': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1 Score': []\n",
    "}\n",
    "\n",
    "# Perform cross-validation for each pipeline\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # Use stratified cross-validation\n",
    "for name, pipeline in pipelines.items():\n",
    "    scores = cross_validate(pipeline, X_train, y_train, cv=cv, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "    metrics['Model'].append(name)\n",
    "    metrics['Accuracy'].append(scores['test_accuracy'].mean())\n",
    "    metrics['Precision'].append(scores['test_precision_weighted'].mean())\n",
    "    metrics['Recall'].append(scores['test_recall_weighted'].mean())\n",
    "    metrics['F1 Score'].append(scores['test_f1_weighted'].mean())\n",
    "\n",
    "# Create DataFrame from metrics\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(\"Cross-Validation Metrics:\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model_name = metrics_df.loc[metrics_df['Accuracy'].idxmax()]['Model']\n",
    "best_model = pipelines[best_model_name]\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(f\"\\nTest Set Evaluation for {best_model_name}:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix for the best model\n",
    "print(f\"Confusion Matrix for {best_model_name}:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Feature importance for interpretable models\n",
    "if hasattr(best_model.named_steps['model'], 'feature_importances_'):\n",
    "    feature_importances = best_model.named_steps['model'].feature_importances_\n",
    "    feature_names = best_model.named_steps['preprocessor'].transformers_[1][1].named_steps['encoder'].get_feature_names_out(cat_features).tolist()\n",
    "    feature_names += num_features\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "    print(\"\\nFeature Importances:\")\n",
    "    print(importance_df.sort_values(by='Importance', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
